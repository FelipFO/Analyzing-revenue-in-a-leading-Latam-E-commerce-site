{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"KQKwdJZdk1qC"},"outputs":[],"source":["# Importing pandas\n","import pandas as pd\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"W8sFPX_ck1qF"},"outputs":[{"name":"stderr","output_type":"stream","text":["Traceback (most recent call last):\n","  File \"<frozen runpy>\", line 198, in _run_module_as_main\n","  File \"<frozen runpy>\", line 88, in _run_code\n","  File \"C:\\Users\\afeli\\anaconda3\\Scripts\\gdown.exe\\__main__.py\", line 7, in <module>\n","  File \"C:\\Users\\afeli\\anaconda3\\Lib\\site-packages\\gdown\\cli.py\", line 156, in main\n","    filename = download(\n","               ^^^^^^^^^\n","  File \"C:\\Users\\afeli\\anaconda3\\Lib\\site-packages\\gdown\\download.py\", line 259, in download\n","    filename_from_url = m.groups()[0]\n","                        ^^^^^^^^\n","AttributeError: 'NoneType' object has no attribute 'groups'\n","Traceback (most recent call last):\n","  File \"<frozen runpy>\", line 198, in _run_module_as_main\n","  File \"<frozen runpy>\", line 88, in _run_code\n","  File \"C:\\Users\\afeli\\anaconda3\\Scripts\\gdown.exe\\__main__.py\", line 7, in <module>\n","  File \"C:\\Users\\afeli\\anaconda3\\Lib\\site-packages\\gdown\\cli.py\", line 156, in main\n","    filename = download(\n","               ^^^^^^^^^\n","  File \"C:\\Users\\afeli\\anaconda3\\Lib\\site-packages\\gdown\\download.py\", line 259, in download\n","    filename_from_url = m.groups()[0]\n","                        ^^^^^^^^\n","AttributeError: 'NoneType' object has no attribute 'groups'\n"]},{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'olist_order_payments_dataset.csv'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgdown \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1SoKNC_Tbd7_ygUntnxErjFP57tJ1KlRR\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgdown \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1AA3N3quwh7emKGvhn5j_NPq1srrA3Mdf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m order_payments \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124molist_order_payments_dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m orders_dataset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124molist_orders_dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[1;32mc:\\Users\\afeli\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n","File \u001b[1;32mc:\\Users\\afeli\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n","File \u001b[1;32mc:\\Users\\afeli\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n","File \u001b[1;32mc:\\Users\\afeli\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1706\u001b[0m     f,\n\u001b[0;32m   1707\u001b[0m     mode,\n\u001b[0;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1714\u001b[0m )\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n","File \u001b[1;32mc:\\Users\\afeli\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n","\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'olist_order_payments_dataset.csv'"]}],"source":["# Reading the csvs\n","!gdown \"1SoKNC_Tbd7_ygUntnxErjFP57tJ1KlRR\"\n","!gdown \"1AA3N3quwh7emKGvhn5j_NPq1srrA3Mdf\"\n","\n","order_payments = pd.read_csv(\"olist_order_payments_dataset.csv\")\n","orders_dataset = pd.read_csv(\"olist_orders_dataset.csv\")"]},{"cell_type":"markdown","metadata":{"id":"9BRauXlMk1qF"},"source":["We will first explore both datasets to see how they look like, showing the first five rows."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wudwt9XBk1qH"},"outputs":[],"source":["# Checking the first five rows of the order_payments content\n","order_payments.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ulvfow4yk1qI"},"outputs":[],"source":["# Checking the first five rows of the order_payments content\n","orders_dataset.head()"]},{"cell_type":"markdown","metadata":{"id":"Db6cSdxPk1qI"},"source":["We will now merge these two datasets with inner join and using a common column on both, order_id. We will visualize how the datasets look like after the merge."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gq-YOj_5k1qJ"},"outputs":[],"source":["# Merging both datasets\n","merged_orders = pd.merge(order_payments, orders_dataset, on= 'order_id', how= 'inner')\n","merged_orders.head()"]},{"cell_type":"markdown","metadata":{"id":"_l_NR635k1qJ"},"source":["There is a method called shape, that allows us to see the shape of the dataset. It will show a tuple of the number of rows, and the number of columns."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BphNv1m8k1qJ"},"outputs":[],"source":["# Showing the shape of merged_orders\n","merged_orders.shape"]},{"cell_type":"markdown","metadata":{"id":"wNdPejEWk1qK"},"source":["Now we will get a glimpse of some general elements of the dataset with the .info() method. It shows the dtype of the index and columns, the total amount of entries (rows) and columns, the total amount of columns per dtype, the non-null values amount per column, and memory usage."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TBbJxuyxk1qK"},"outputs":[],"source":["# Getting the database info\n","merged_orders.info()"]},{"cell_type":"markdown","metadata":{"id":"rnnNNxlWk1qL"},"source":["There is another method, .describe(); that shows us the following values for numeric columns (float or numeric, in this case there are 3) unless we pass the 'all' to the parameter include: The total count of elements, the mean, the standard deviation, the minimum and maximum values and the 25th, 50th and 75th percentiles."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pm7b9OtOk1qL"},"outputs":[],"source":["# Getting the database description\n","merged_orders.describe()"]},{"cell_type":"markdown","metadata":{"id":"3knWMDmYk1qL"},"source":["We have seen that with the .info() method we get the total number of non-nulls, and if there is an inconsistency among those numbers, it means there are some null values. And the way to get the total amount of nulls is first applying the .isna() method that helps us detect them, and then we add all those occurrences with .sum()."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hxQQCHvhk1qM"},"outputs":[],"source":["# Getting the total amount of null values per column\n","merged_orders.isna().sum()"]},{"cell_type":"markdown","metadata":{"id":"zzgR2W6Nk1qM"},"source":["There are 3 columns with several nulls. In this case, we are going to drop them with the .dropna() method. We can drop the rows with null values (axis = 0) or the columns (axis = 1).\n","\n","Let's drop the rows that have NAs and check again if they were eliminated. There should be zero nulls."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SgPaPKKck1qN"},"outputs":[],"source":["# Dropping the NAs and checking if they were eliminated\n","merged_orders.dropna(axis= 0, inplace= True)\n","\n","merged_orders.isna().sum()"]},{"cell_type":"markdown","metadata":{"id":"3Qb8Udgvk1qN"},"source":["Our goal now is to create a new column with the average payment value per installment. But before that, if we pay attention to the payment_installments column information given by the .describe() method, we will see that the minimum value is 0, and math tells us that we can't divide a number by 0.\n","\n","So what we will do right now, is to replace all those 0s by 1s on that column, using the .replace() method."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Igzj0cQ1k1qN"},"outputs":[],"source":["# Replacing the 0s by 1s\n","merged_orders['payment_installments'] = merged_orders['payment_installments'].replace(0, 1)"]},{"cell_type":"markdown","metadata":{"id":"hiFRt2Uik1qN"},"source":["How can we check if the 0s are gone? We can use describe again, but there is a neater way to see that, by using the .unique() method. This shows us the unique elements present.\n","\n","So we are going to use that method on the payment_installments column to see them, and no 0 will be shown."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"706hp4Y8k1qO"},"outputs":[],"source":["# Showing the unique elements of the payment_isntallments column\n","merged_orders['payment_installments'].unique()"]},{"cell_type":"markdown","metadata":{"id":"Xdr_UmCVk1qO"},"source":["And there is a way to know how many unique elements that column has: the .nunique() method."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FDfbZAkvk1qO"},"outputs":[],"source":["# Showing the amount of unique elements of the payment_isntallments column\n","merged_orders['payment_installments'].nunique()"]},{"cell_type":"markdown","metadata":{"id":"xJktEz5yk1qO"},"source":["Now we can move on to create a new column that will have the payment value per installment. For this we can use the .apply() method. This is used to apply a function along an axis.\n","\n","We will also show the new values on that newly created column."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eT8AYFfNk1qP"},"outputs":[],"source":["# Creating new column with the payment per installment and showing its content\n","merged_orders['payment_per_installment'] = merged_orders.apply(\n","    lambda row: row.payment_value / row.payment_installments, axis=1\n","    )\n","\n","merged_orders['payment_per_installment']"]},{"cell_type":"markdown","metadata":{"id":"S9yxqyd0k1qP"},"source":["Now let's check the unique elements of the payment_type column"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P61Z_MHtk1qP"},"outputs":[],"source":["# Checking the unique elements of the payment_type column\n","merged_orders['payment_type'].unique()"]},{"cell_type":"markdown","metadata":{"id":"lVPRYoX-k1qP"},"source":["There are four elements, and there are three in English and one in Portuguese. To be consistent, we will change the Portuguese word \"boleto\" by its English word \"ticket\".\n","\n","For this we can use the .loc() method. With this we can access a group of rows and columns by label or labels or a boolean array.\n","\n","After that, we will check the unique elements of that column again and \"boleto\" should have been replaced by \"ticket\"."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"upT9lZuhk1qQ"},"outputs":[],"source":["# Changing boleto in Portuguese for ticket in English and showing how the word was replaced\n","merged_orders.loc[(merged_orders.payment_type == 'boleto'),'payment_type']='ticket'\n","merged_orders['payment_type'].unique()"]},{"cell_type":"markdown","metadata":{"id":"w0HqbI3Ik1qQ"},"source":["Antoher thing we can do is count the amount of values per element. For this we can use the .value_counts() method. Applied to the payment type it will show us the amount of occurrences of each of the four elements."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EoKgT0kBk1qQ"},"outputs":[],"source":["# Showing the amount of occurrences per element for the payment_type column\n","merged_orders['payment_type'].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"VwY9OiV1k1qQ"},"source":["Or we can see the amount of total payment values per element. For this we have the .groupby() method that allows us to group the dataframe by some criteria. We want to group them by the payment type."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n9p9g65Mk1qQ"},"outputs":[],"source":["# Grouping the total payment values by payment_type\n","merged_orders.groupby(by='payment_type').sum()['payment_value']"]},{"cell_type":"markdown","metadata":{"id":"8yR6pKbwk1qQ"},"source":["We have used the .loc() method before. But there is a similar one called .iloc(). It lets us index what we want by the integer location.\n","\n","Let's see the first row of the dataset using this method."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T6hK0hbok1qR"},"outputs":[],"source":["# Checking the first row of the dataset with .iloc()\n","merged_orders.iloc[[0]]\n"]},{"cell_type":"markdown","metadata":{"id":"E0wVTgzKk1qR"},"source":["We can also do the same thing but for the column. for that we will call all the rows (using \":\") and the first column by its position, 0."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w9N3cqXQk1qR"},"outputs":[],"source":["# Checking the information of the first column using .loc()\n","merged_orders.iloc[:,[0]]"]},{"cell_type":"markdown","metadata":{"id":"Y6uRjBAxk1qR"},"source":["We can also index a specific value, for example the first element of the first column"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_JZL7Ybpk1qR"},"outputs":[],"source":["# Showing the first element of the first column with .iloc()\n","merged_orders.iloc[[0],[0]]"]},{"cell_type":"markdown","metadata":{"id":"NSqSI0T2k1qR"},"source":["Different amount of rows or columns can be shown with this method as well."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nOe32dOok1qS"},"outputs":[],"source":["# Showing the first three rows of the dataset with .iloc()\n","merged_orders.iloc[0:3]"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
